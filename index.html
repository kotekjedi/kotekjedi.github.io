<!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <title>Alexander Panfilov - AI Safety Research</title>
            <meta name="description" content="AI safety, Adversarial ML, & LLM Red-Teaming">
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600&display=swap" rel="stylesheet">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <link rel="stylesheet" href="assets/styles.css">
            <link rel="icon" type="image/x-icon" href="assets/favicon_mine.ico">
            <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4SLC5348B5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-4SLC5348B5');
</script>
<script data-goatcounter="https://kotekjedi.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
            <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Alexander Panfilov",
  "jobTitle": "PhD Student",
  "description": "AI safety, Adversarial ML, & LLM Red-Teaming",
  "affiliation": {
    "@type": "Organization",
    "name": "ELLIS Institute Tuebingen",
    "alternateName": "IMPRS-IS"
  },
  "url": "https://kotekjedi.github.io",
  "image": "https://kotekjedi.github.io/assets/img/profile_mine_new.jpg",
  "sameAs": [
    "https://scholar.google.com/citations?user=M65_TPEAAAAJ&hl=en",
    "https://x.com/kotekjedi_ml",
    "https://www.linkedin.com/in/kotekjedi",
    "https://github.com/kotekjedi"
  ],
  "email": "kotekjedi@gmail.com",
  "workLocation": {
    "@type": "Place",
    "name": "Tuebingen, Germany"
  },
  "publication": [
    {
      "@type": "ScholarlyArticle",
      "headline": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
      "author": [
        {
          "@type": "Person",
          "name": "Mikhail Terekhov*"
        },
        {
          "@type": "Person",
          "name": "Alexander Panfilov*"
        },
        {
          "@type": "Person",
          "name": "Daniil Dzenhaliou*"
        },
        {
          "@type": "Person",
          "name": "Caglar Gulcehre"
        },
        {
          "@type": "Person",
          "name": "Maksym Andriushchenko"
        },
        {
          "@type": "Person",
          "name": "Ameya Prabhu"
        },
        {
          "@type": "Person",
          "name": "Jonas Geiping"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "preprint"
      },
      "datePublished": "2025",
      "url": "https://arxiv.org/abs/2510.09462"
    },
    {
      "@type": "ScholarlyArticle",
      "headline": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs",
      "author": [
        {
          "@type": "Person",
          "name": "Alexander Panfilov*"
        },
        {
          "@type": "Person",
          "name": "Evgenii Kortukov*"
        },
        {
          "@type": "Person",
          "name": "Kristina Nikolic"
        },
        {
          "@type": "Person",
          "name": "Matthias Bethge"
        },
        {
          "@type": "Person",
          "name": "Sebastian Lapuschkin"
        },
        {
          "@type": "Person",
          "name": "Wojciech Samek"
        },
        {
          "@type": "Person",
          "name": "Ameya Prabhu"
        },
        {
          "@type": "Person",
          "name": "Maksym Andriushchenko"
        },
        {
          "@type": "Person",
          "name": "Jonas Geiping"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "preprint"
      },
      "datePublished": "2025",
      "url": "https://arxiv.org/abs/2509.18058"
    },
    {
      "@type": "ScholarlyArticle",
      "headline": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
      "author": [
        {
          "@type": "Person",
          "name": "Egor Zverev"
        },
        {
          "@type": "Person",
          "name": "Evgenii Kortukov"
        },
        {
          "@type": "Person",
          "name": "Alexander Panfilov"
        },
        {
          "@type": "Person",
          "name": "Soroush Tabesh"
        },
        {
          "@type": "Person",
          "name": "Alexandra Volkova"
        },
        {
          "@type": "Person",
          "name": "Sebastian Lapuschkin"
        },
        {
          "@type": "Person",
          "name": "Wojciech Samek"
        },
        {
          "@type": "Person",
          "name": "Christoph Lampert"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "BuildingTrust Workshop at ICLR 2025"
      },
      "datePublished": "2025",
      "url": "https://arxiv.org/abs/2503.10566"
    },
    {
      "@type": "ScholarlyArticle",
      "headline": "Capability-Based Scaling Laws for LLM Red-Teaming",
      "author": [
        {
          "@type": "Person",
          "name": "Alexander Panfilov"
        },
        {
          "@type": "Person",
          "name": "Paul Kassianik"
        },
        {
          "@type": "Person",
          "name": "Maksym Andriushchenko"
        },
        {
          "@type": "Person",
          "name": "Jonas Geiping"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "R2-FM Workshop at ICML 2025"
      },
      "datePublished": "2025",
      "url": "https://arxiv.org/abs/2505.20162"
    },
    {
      "@type": "ScholarlyArticle",
      "headline": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks",
      "author": [
        {
          "@type": "Person",
          "name": "Valentyn Boreiko*"
        },
        {
          "@type": "Person",
          "name": "Alexander Panfilov*"
        },
        {
          "@type": "Person",
          "name": "Vaclav Voracek"
        },
        {
          "@type": "Person",
          "name": "Matthias Hein"
        },
        {
          "@type": "Person",
          "name": "Jonas Geiping"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "Proceedings of the 42nd International Conference on Machine Learning"
      },
      "datePublished": "2025",
      "url": "https://proceedings.mlr.press/v267/boreiko25a.html"
    },
    {
      "@type": "ScholarlyArticle",
      "headline": "Provable Compositional Generalization for Object-Centric Learning",
      "author": [
        {
          "@type": "Person",
          "name": "Thaddaeus Wiedemer*"
        },
        {
          "@type": "Person",
          "name": "Jack Brady*"
        },
        {
          "@type": "Person",
          "name": "Alexander Panfilov*"
        },
        {
          "@type": "Person",
          "name": "Attila Juhos*"
        },
        {
          "@type": "Person",
          "name": "Matthias Bethge"
        },
        {
          "@type": "Person",
          "name": "Wieland Brendel"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "ICLR 2024"
      },
      "datePublished": "2024",
      "url": "https://openreview.net/forum?id=7VPTUWkiDQ"
    }
  ]
}
            </script>
        </head>
        <body>
            <nav class="site-nav"><div class="brand">Sasha&apos;s Website</div><div class="nav-links"><a href="#news">News</a><a href="#research">Research</a></div></nav>
            <header class="hero" id="top">
                <div class="hero-grid">
                    <div class="hero-content">
                        <p class="eyebrow">ELLIS Institute / IMPRS-IS, Tuebingen</p>
                        <h1>Alexander <span>Panfilov</span></h1>
                        <p class="tagline">AI safety, Adversarial ML, & LLM Red-Teaming</p>
                        <div class="social-row">
                            <a class="social-link" href="https://scholar.google.com/citations?user=M65_TPEAAAAJ&hl=en" target="_blank" rel="noopener"><img src="assets/icons8-google-scholar.svg" alt="Scholar icon" loading="lazy"><span>Scholar</span></a>
<a class="social-link" href="https://x.com/kotekjedi_ml" target="_blank" rel="noopener"><i class="fa-brands fa-x-twitter"></i><span>Twitter</span></a>
<a class="social-link" href="https://www.linkedin.com/in/kotekjedi" target="_blank" rel="noopener"><i class="fab fa-linkedin"></i><span>LinkedIn</span></a>
<a class="social-link" href="https://github.com/kotekjedi" target="_blank" rel="noopener"><i class="fab fa-github"></i><span>GitHub</span></a>
<a class="social-link" href="mailto:kotekjedi@gmail.com" target="_blank" rel="noopener"><i class="fa-solid fa-envelope"></i><span>Email</span></a>
                        </div>
                        <p>Yo! My name is Sasha and I am a second-year ELLIS / IMPRS-IS PhD student in Tuebingen advised by Jonas Geiping and Maksym Andriushchenko.</p>
<p>I work on AI Safety, particularly on red-teaming LLMs and stuff around them. Roughly two days a week I am an AI doomer.</p>
<p>Previously I was obsessed with LLM jailbreaks, trying to understand realistic threat models, whether current safety evaluations are trustworthy, and whether we’re doomed. Lately I’m leaning into white-box alignment methods and AI control.</p>
<p>I am open to collaboration - feel free to drop an email! I’m also looking for a safety/security internship opportunity in 2026/2027.</p>
                        <div class="cta-row">
                            <a class="pill-button primary" href="assets/pdf/cv.pdf" target="_blank" rel="noopener">Download CV</a>
                            <a class="pill-button secondary" href="mailto:kotekjedi@gmail.com">Email me</a>
                        </div>
                    </div>
                    <div class="hero-photo">
                        <img src="assets/img/profile_mine_new.jpg" alt="Alexander Panfilov" loading="lazy">
                    </div>
                </div>
            </header>

            <main>
                <section class="panel focus-panel" aria-labelledby="focus-title">
                    <div class="panel-heading">
                        <h2 id="focus-title">My current...</h2>
                    </div>
                    <div class="focus-grid">
                        <div class="focus-card">
    <h4>Research Interests</h4>
    <p>I find introspection capability of LLMs fascinating and think it can be used as a way to reason about white-box interventions and useful for self-jailbreaking.</p>
</div>
<div class="focus-card">
    <h4>Whereabouts</h4>
    <p>Based in London from January through March for MATS. Happy to grab a coffee if you are around!</p>
</div>
<div class="focus-card">
    <h4>Plans</h4>
    <p>Planning to attend ICLR 2026 in Brazil. Happy to catch up there!</p>
</div>
                    </div>
                </section>

                <section class="panel news-panel" id="news" aria-labelledby="news-title">
                    <div class="panel-heading">
                        <h2 id="news-title">News & updates</h2>
                    </div>
                    <ul class="news-timeline">
                        <li class="news-item">
    <span class="news-date">Dec 09, 2025</span>
    <div class="news-body">I will join MATS 9.0 cohort as a part of GDM stream (Zimermann/Lindner/Emmons/Jenner) focusing on red-teaming of whit-box detectors!</div>
</li>
<li class="news-item">
    <span class="news-date">Sep 01, 2025</span>
    <div class="news-body"><a href="https://scholar.google.com/citations?user=aFpbzMYAAAAJ&hl=en" target="_blank">Kristina Nikolić</a>, <a href="https://scholar.google.com/citations?user=7qTZ4NEAAAAJ&hl=en" target="_blank">Evgenii Kortukov</a>, and I won third place at the <span style="  color: #2563eb;">ARENA 6.0 Mechanistic Interpretability Hackathon</span> by Apart Research in LISA (London)!</div>
</li>
<li class="news-item">
    <span class="news-date">Jul 09, 2025</span>
    <div class="news-body"><em>Capability-Based Scaling Laws for LLM Red-Teaming</em> accepted at <span style=" color: #2563eb;"><strong>ICML 2025</strong> Workshop on Reliable and Responsible Foundation Models</span>!</div>
</li>
<li class="news-item">
    <span class="news-date">Jun 23, 2025</span>
    <div class="news-body">Presented our work <em>Capability-Based Scaling Laws for LLM Red-Teaming</em> and <em>ASIDE</em> at the <span style="  color: #2563eb;">Google's Red Teaming seminar</span>. You can find the slides <a href="assets/talks/google_talk.pdf" target="_blank">here</a>. Thanks for the invitation!</div>
</li>
<li class="news-item">
    <span class="news-date">May 01, 2025</span>
    <div class="news-body">Our work, <em>An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</em>, has been accepted at <span style="  color: #2563eb;"><strong>ICML 2025</strong></span>.</div>
</li>
<li class="news-item">
    <span class="news-date">Apr 15, 2025</span>
    <div class="news-body">Our work, <em>ASIDE: Architectural Separation of Instructions and Data in Language Models</em>, has been accepted for an <strong>oral</strong> presentation at the <span style="  color: #2563eb;">BuildingTrust Workshop at <strong>ICLR 2025</strong></span>.</div>
</li>
<li class="news-item">
    <span class="news-date">Nov 05, 2024</span>
    <div class="news-body">Presented our work, <em>Provable Compositional Generalization for Object-Centric Learning</em> at <span style="  color: #2563eb;">EPFL</span> (Nicolas Flammarion's group seminar). You can find the slides <a href="assets/talks/ocl_epfl_talk.pdf" target="_blank">here</a>.</div>
</li>
<li class="news-item">
    <span class="news-date">Oct 09, 2024</span>
    <div class="news-body">Our work, <em>A Realistic Threat Model for Large Language Model Jailbreaks</em>, has been accepted for an <strong>oral</strong> presentation at the <span style=" color: #2563eb;">Red Teaming GenAI Workshop at <strong>NeurIPS 2024</strong></span>.</div>
</li>
<li class="news-item">
    <span class="news-date">May 01, 2024</span>
    <div class="news-body">Started my PhD at the ELLIS Institute Tübingen / Max Planck Institute for Intelligent Systems. You can find the slides for my <span style=" color: #2563eb;">IMPRS</span> talk <a href="assets/talks/imprs_talk.pdf" target="_blank">here</a>.</div>
</li>
                    </ul>
                </section>

                <section class="panel" id="research" aria-labelledby="research-title">
                    <div class="panel-heading">
                        <h2 id="research-title">Research</h2>
                        <p class="panel-description">Some of my recent work :) </p>
                    </div>
                    <div class="publications">
                        <article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/monitor.png" alt="Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-terekhov2025monitor">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue">preprint</span> | 2025</div>
    <h4 class="pub-title"><a href="https://arxiv.org/abs/2510.09462" target="_blank" rel="noopener">Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols</a></h4>
    <p class="pub-authors">Mikhail Terekhov*, <span class="author-self">Alexander Panfilov*</span>, Daniil Dzenhaliou*, Caglar Gulcehre, Maksym Andriushchenko, Ameya Prabhu, Jonas Geiping</p>
    <div class="pub-actions"><a class="pill-button" href="https://arxiv.org/abs/2510.09462" target="_blank" rel="noopener">Paper</a>
<a class="pill-button" href="https://mikhailterekhov.github.io/control-adaptive-attacks/" target="_blank" rel="noopener">Website</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-terekhov2025monitor">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-terekhov2025monitor">
    <p>AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-n attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-terekhov2025monitor">
                <pre class="pub-bibtex"><code>@inproceedings{terekhov2025monitor,
    author = &quot;Terekhov*, Mikhail and Panfilov*, Alexander and Dzenhaliou*, Daniil and Gulcehre, Caglar and Andriushchenko, Maksym and Prabhu, Ameya and Geiping, Jonas&quot;,
    title = &quot;Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols&quot;,
    booktitle = &quot;preprint&quot;,
    url = &quot;https://arxiv.org/abs/2510.09462&quot;,
    year = &quot;2025&quot;,
}</code></pre>
            </div>
  </div>
</article>
<article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/faking.png" alt="Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-panfilov2025dishonesty">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue">preprint</span> | 2025</div>
    <h4 class="pub-title"><a href="https://arxiv.org/abs/2509.18058" target="_blank" rel="noopener">Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs</a></h4>
    <p class="pub-authors"><span class="author-self">Alexander Panfilov*</span>, Evgenii Kortukov*, Kristina Nikolic, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping</p>
    <div class="pub-actions"><a class="pill-button" href="https://arxiv.org/abs/2509.18058" target="_blank" rel="noopener">Paper</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-panfilov2025dishonesty">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-panfilov2025dishonesty">
    <p>Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are crafted to be subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using them as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-panfilov2025dishonesty">
                <pre class="pub-bibtex"><code>@inproceedings{panfilov2025dishonesty,
    author = &quot;Panfilov*, Alexander and Kortukov*, Evgenii and Nikolic, Kristina and Bethge, Matthias and Lapuschkin, Sebastian and Samek, Wojciech and Prabhu, Ameya and Andriushchenko, Maksym and Geiping, Jonas&quot;,
    title = &quot;Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs&quot;,
    booktitle = &quot;preprint&quot;,
    url = &quot;https://arxiv.org/abs/2509.18058&quot;,
    year = &quot;2025&quot;,
}</code></pre>
            </div>
  </div>
</article>
<article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/aside.png" alt="ASIDE: Architectural Separation of Instructions and Data in Language Models cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-zverev2025aside">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue">BuildingTrust Workshop at <strong>ICLR 2025</strong></span><span class="badge-pill badge-oral">oral</span></div>
    <h4 class="pub-title"><a href="https://arxiv.org/abs/2503.10566" target="_blank" rel="noopener">ASIDE: Architectural Separation of Instructions and Data in Language Models</a></h4>
    <p class="pub-authors">Egor Zverev, Evgenii Kortukov, <span class="author-self">Alexander Panfilov</span>, Soroush Tabesh, Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph Lampert</p>
    <div class="pub-actions"><a class="pill-button" href="https://arxiv.org/abs/2503.10566" target="_blank" rel="noopener">Paper</a>
<a class="pill-button" href="https://github.com/egozverev/aside" target="_blank" rel="noopener">Code</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-zverev2025aside">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-zverev2025aside">
    <p>Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at \href{https://github.com/egozverev/aside}{https://github.com/egozverev/aside}.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-zverev2025aside">
                <pre class="pub-bibtex"><code>@inproceedings{zverev2025aside,
    author = &quot;Zverev, Egor and Kortukov, Evgenii and Panfilov, Alexander and Tabesh, Soroush and Volkova, Alexandra and Lapuschkin, Sebastian and Samek, Wojciech and Lampert, Christoph H&quot;,
    title = &quot;ASIDE: Architectural Separation of Instructions and Data in Language Models&quot;,
    booktitle = &quot;BuildingTrust Workshop at ICLR 2025&quot;,
    url = &quot;https://arxiv.org/abs/2503.10566&quot;,
    year = &quot;2025&quot;,
}</code></pre>
            </div>
  </div>
</article>
<article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/scaling.png" alt="Capability-Based Scaling Laws for LLM Red-Teaming cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-panfilov2025scalinglaws">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue">R2-FM Workshop at <strong>ICML 2025</strong></span></div>
    <h4 class="pub-title"><a href="https://arxiv.org/abs/2505.20162" target="_blank" rel="noopener">Capability-Based Scaling Laws for LLM Red-Teaming</a></h4>
    <p class="pub-authors"><span class="author-self">Alexander Panfilov</span>, Paul Kassianik, Maksym Andriushchenko, Jonas Geiping</p>
    <div class="pub-actions"><a class="pill-button" href="https://arxiv.org/abs/2505.20162" target="_blank" rel="noopener">Paper</a>
<a class="pill-button" href="https://github.com/kotekjedi/capability-based-scaling" target="_blank" rel="noopener">Code</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-panfilov2025scalinglaws">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-panfilov2025scalinglaws">
    <p>As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a \emph{weak-to-strong} problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the \emph{capability gap} between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target’s capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a \emph{jailbreaking scaling law} that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-panfilov2025scalinglaws">
                <pre class="pub-bibtex"><code>@inproceedings{panfilov2025scalinglaws,
    author = &quot;Panfilov, Alexander and Kassianik, Paul and Andriushchenko, Maksym and Geiping, Jonas&quot;,
    title = &quot;Capability-Based Scaling Laws for LLM Red-Teaming&quot;,
    booktitle = &quot;R2-FM Workshop at ICML 2025&quot;,
    url = &quot;https://arxiv.org/abs/2505.20162&quot;,
    year = &quot;2025&quot;,
}</code></pre>
            </div>
  </div>
</article>
<article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/threat_model.png" alt="An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-boreiko2024athreatmodel">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue"><strong>ICML 2025</strong></span></div>
    <h4 class="pub-title"><a href="https://proceedings.mlr.press/v267/boreiko25a.html" target="_blank" rel="noopener">An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</a></h4>
    <p class="pub-authors">Valentyn Boreiko*, <span class="author-self">Alexander Panfilov*</span>, Vaclav Voracek, Matthias Hein, Jonas Geiping</p>
    <div class="pub-actions"><a class="pill-button" href="https://proceedings.mlr.press/v267/boreiko25a.html" target="_blank" rel="noopener">Paper</a>
<a class="pill-button" href="https://github.com/valentyn1boreiko/llm-threat-model" target="_blank" rel="noopener">Code</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-boreiko2024athreatmodel">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-boreiko2024athreatmodel">
    <p>A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, nonparametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After an extensive comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent bigrams, either selecting the ones absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-boreiko2024athreatmodel">
                <pre class="pub-bibtex"><code>@inproceedings{boreiko2024athreatmodel,
    author = &quot;Boreiko*, Valentyn and Panfilov*, Alexander and Voracek, Vaclav and Hein, Matthias and Geiping, Jonas&quot;,
    title = &quot;An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks&quot;,
    booktitle = &quot;Proceedings of the 42nd International Conference on Machine Learning&quot;,
    display = &quot;ICML 2025&quot;,
    url = &quot;https://proceedings.mlr.press/v267/boreiko25a.html&quot;,
    year = &quot;2025&quot;,
}</code></pre>
            </div>
  </div>
</article>
<article class="publication-card">
  <div class="pub-thumb">
    <img src="assets/img/publications/consistency_v1.png" alt="Provable Compositional Generalization for Object-Centric Learning cover" loading="lazy">
    <button class="pill-button thumb-button" data-toggle-target="abstract-wiedemer2024provable">Abstract</button>
  </div>
  <div class="pub-body">
    <div class="pub-meta"><span class="venue"><strong>ICLR 2024</strong></span><span class="badge-pill badge-oral">oral</span></div>
    <h4 class="pub-title"><a href="https://openreview.net/forum?id=7VPTUWkiDQ" target="_blank" rel="noopener">Provable Compositional Generalization for Object-Centric Learning</a></h4>
    <p class="pub-authors">Thaddaeus Wiedemer*, Jack Brady*, <span class="author-self">Alexander Panfilov*</span>, Attila Juhos*, Matthias Bethge, Wieland Brendel</p>
    <div class="pub-actions"><a class="pill-button" href="https://openreview.net/forum?id=7VPTUWkiDQ" target="_blank" rel="noopener">Paper</a>
<a class="pill-button" href="https://brendel-group.github.io/objects-compositional-generalization/" target="_blank" rel="noopener">Website</a>
<a class="pill-button" href="https://github.com/brendel-group/objects-compositional-generalization" target="_blank" rel="noopener">Code</a>
      <button class="pill-button ghost" data-toggle-target="bibtex-wiedemer2024provable">BibTeX</button>
    </div>
<div class="toggle-panel" id="abstract-wiedemer2024provable">
    <p>Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.</p>
</div>
<div class="toggle-panel toggle-panel-bib" id="bibtex-wiedemer2024provable">
                <pre class="pub-bibtex"><code>@inproceedings{wiedemer2024provable,
    author = &quot;Wiedemer*, Thaddaeus and Brady*, Jack and Panfilov*, Alexander and Juhos*, Attila and Bethge, Matthias and Brendel, Wieland&quot;,
    title = &quot;Provable Compositional Generalization for Object-Centric Learning&quot;,
    booktitle = &quot;ICLR 2024&quot;,
    url = &quot;https://openreview.net/forum?id=7VPTUWkiDQ&quot;,
    year = &quot;2024&quot;,
}</code></pre>
            </div>
  </div>
</article>
                    </div>
                </section>

                <section class="panel" id="contact">
                    <div class="panel-heading">
                        <h3 class="panel-title-sm">Acknowledgements</h3>
                    </div>
                    <p>I am grateful to the many friends and colleagues, from whom I learned so much, for their invaluable guidance
and for shaping my research vision. I would like to especially acknowledge
<a href="https://www.linkedin.com/in/svyatoslav-oreshin/" target="_blank">Svyatoslav Oreshin</a>,
<a href="https://scholar.google.com/citations?user=wcdrgdYAAAAJ&hl=en" target="_blank">Arip Asadualev</a>,
<a href="https://scholar.google.de/citations?user=4jdISHwAAAAJ&hl=en" target="_blank">Roland Zimmermann</a>,
<a href="https://scholar.google.com/citations?user=aeCiRSYAAAAJ&hl=en" target="_blank">Thaddaeus Wiedemer</a>,
<a href="https://scholar.google.com/citations?hl=en&user=jgPzOmgAAAAJ" target="_blank">Jack Brady</a>,
<a href="https://scholar.google.com/citations?user=v-JL-hsAAAAJ&hl=en" target="_blank">Wieland Brendel</a>,
<a href="https://scholar.google.com/citations?hl=en&user=gzRuY4cAAAAJ" target="_blank">Valentyn Boreiko</a>,
<a href="https://scholar.google.com/citations?user=0ZAb3tsAAAAJ&hl=en" target="_blank">Matthias Hein</a>,
<a href="https://scholar.google.com/citations?hl=en&user=exaNV-0AAAAJ" target="_blank">Shashwat Goel</a>,
<a href="https://scholar.google.com/citations?hl=en&user=e-YbZyEAAAAJ" target="_blank">Illia Shumailov</a>,
<a href="https://scholar.google.com/citations?user=ZNtuJYoAAAAJ" target="_blank">Maksym Andriushchenko</a>, and
<a href="https://scholar.google.de/citations?user=206vNCEAAAAJ&hl=en" target="_blank">Jonas Geiping</a>.</p>
                </section>
            </main>

            <footer class="site-footer">
                <p>Vibe-coded with CodeX. Last updated Dec 10, 2025.</p>
            </footer>

            <script>
            document.querySelectorAll('[data-toggle-target]').forEach((button) => {
                button.addEventListener('click', () => {
                    const targetId = button.dataset.toggleTarget;
                    const target = document.getElementById(targetId);
                    if (!target) return;
                    target.classList.toggle('is-visible');
                    const expanded = target.classList.contains('is-visible');
                    button.setAttribute('aria-expanded', expanded);

                    const isBibtex = targetId.startsWith('bibtex-');
                    const isAbstract = targetId.startsWith('abstract-');
                    const siblingPrefix = isBibtex ? 'abstract-' : isAbstract ? 'bibtex-' : null;

                    if (siblingPrefix) {
                        const siblingId = targetId.replace(isBibtex ? 'bibtex-' : 'abstract-', siblingPrefix);
                        const siblingPanel = document.getElementById(siblingId);
                        if (siblingPanel && siblingPanel.classList.contains('is-visible')) {
                            siblingPanel.classList.remove('is-visible');
                            const siblingButton = document.querySelector(`[data-toggle-target="${siblingId}"]`);
                            if (siblingButton) {
                                siblingButton.setAttribute('aria-expanded', 'false');
                            }
                        }
                    }
                });
            });
            </script>
            <script data-goatcounter="https://kotekjedi.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
        </body>
        </html>